{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. # 🛠 Install Libraries","metadata":{}},{"cell_type":"markdown","source":"## For PC","metadata":{}},{"cell_type":"code","source":"#!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n#!pip install --user numpy \n#!pip install --user pandas \n#!pip install  segmentation-models-pytorch\n# !python -m pip install opencv-python\n# !pip install tensorflow\n# !pip install -q scikit-learn==1.0\n#!pip install plotly\n# !pip install --user albumentations\n# import sys  \n# !{sys.executable} -m pip install --user matplotlib\n#!pip install ipywidgets --user\n#!pip install -U albumentations[imgaug]","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:23.649859Z","iopub.execute_input":"2022-07-21T16:01:23.650507Z","iopub.status.idle":"2022-07-21T16:01:23.729976Z","shell.execute_reply.started":"2022-07-21T16:01:23.650469Z","shell.execute_reply":"2022-07-21T16:01:23.728176Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## For Kaggle !!","metadata":{}},{"cell_type":"code","source":"!pip install  segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:23.732254Z","iopub.execute_input":"2022-07-21T16:01:23.732660Z","iopub.status.idle":"2022-07-21T16:01:35.197882Z","shell.execute_reply.started":"2022-07-21T16:01:23.732619Z","shell.execute_reply":"2022-07-21T16:01:35.196237Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# 📚 Import Libraries  \n","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:35.200385Z","iopub.execute_input":"2022-07-21T16:01:35.201261Z","iopub.status.idle":"2022-07-21T16:01:35.299288Z","shell.execute_reply.started":"2022-07-21T16:01:35.201216Z","shell.execute_reply":"2022-07-21T16:01:35.297824Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\nimport segmentation_models_pytorch as smp\nimport random\nfrom glob import glob\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\n#import joblib\n#from collections import defaultdict\nfrom IPython import display as ipd\nfrom PIL import Image\n# visualization\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Sklearn\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_  = Fore.GREEN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n# gc\nimport gc","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-07-21T16:01:35.301739Z","iopub.execute_input":"2022-07-21T16:01:35.302205Z","iopub.status.idle":"2022-07-21T16:01:35.394366Z","shell.execute_reply.started":"2022-07-21T16:01:35.302157Z","shell.execute_reply":"2022-07-21T16:01:35.393132Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## versions","metadata":{}},{"cell_type":"code","source":"print(f'Torch version{torch.__version__}')\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))\nimport platform\nprint(f\"Python version: {platform.python_version()}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:35.398536Z","iopub.execute_input":"2022-07-21T16:01:35.398923Z","iopub.status.idle":"2022-07-21T16:01:35.484241Z","shell.execute_reply.started":"2022-07-21T16:01:35.398891Z","shell.execute_reply":"2022-07-21T16:01:35.482645Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# ⚙️ Configuration ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    JUST_PREDICT  = True\n    Kaggle        = True \n    DEBUG         = False\n    wandb_on      = False\n    seed          = 101\n    MULTIMODEL    = True\n    #exp_name      = 'Baselinev2'\n    #comment       = 'unet-efficientnet_b1-224x224-aug2-split2'\n    model_name_1    = 'u-efficientnet-b1'\n    model_name_2    = 'u-efficientnet-b2'\n    model_name_3    = 'u-timm-mobilenetv3_small_minimal_100'\n    weights       = 'imagenet'\n    backbone_1    = 'efficientnet-b1'\n    backbone_2    = 'efficientnet-b2' \n    backbone_3    = 'timm-mobilenetv3_small_minimal_100'\n    models        = []\n    optimizers    = []\n################################################### \n    num_of_models = 1\n    model_number  = 8\n    train_bs      = 12\n    valid_bs      = 12\n    number_imgs   = 100 if DEBUG else 8203     #8203\n    num_test      = 10 if DEBUG else 1000      # 1000\n    print_every   = 8  if DEBUG else 100      #500\n    img_size      = [256, 256] #[540, 960]\n    start_width   = 512\n    start_height  = 512\n    final_width   = 512\n    final_height  = 512\n    epochs        = 4  if DEBUG else 28        #35\n    ###############################################\n    crop_koef     = 1\n    lr            = 2e-3\n    num_workers   = 4 if Kaggle else 0\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 1e-6\n    T_max         = int(30000/train_bs*epochs)+50\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 0 #1e-6\n    n_accumulate  = max(1, 32//train_bs)\n    n_fold        = 5\n    num_classes   = 4\n    classes       = [0,6,7,10]\n    activation    = None #'softmax'\n    device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    images_path   = \"../input/russian-railways-2/images/images/\" if Kaggle else \"./train/images/\" #\"../Цифровой прорыв 2022_Лето\\train\\images\"\n    masks_path    = \"../input/russian-railways-2/mask/mask/\" if Kaggle else  \"./train/mask/\"\n    test_path     = \"../input/russian-railways-2/test/test/\" if Kaggle else \"./test/\"\n    save_path     = '../working/result/' if Kaggle else \"./result/\"\n    best_model_w_1= '../input/russian-railways-2/best_epoch_ofu-efficientnet-b1_v2.bin' if Kaggle else './last_epoch_ofu-efficientnet-b1_v2.bin'\n    best_model_w_2= '../input/russian-railways-2/best_epoch_ofu-efficientnet-b2_v2.bin' if Kaggle else './last_epoch_ofu-efficientnet-b2_v2.bin'\n    best_model_w_3= '../input/russian-railways-2/best_epoch_ofu-timm-mobilenetv3_small_minimal_100_v2.bin' if Kaggle else './best_epoch_ofu-timm-mobilenetv3_small_minimal_100_v2.bin'\n    best_model_w_4= None\n    best_model_w_5= None\n    best_model_w_6= None\n    \n#'../input/russian-railways-2/best_epoch.bin'","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:35.486379Z","iopub.execute_input":"2022-07-21T16:01:35.486825Z","iopub.status.idle":"2022-07-21T16:01:35.584308Z","shell.execute_reply.started":"2022-07-21T16:01:35.486786Z","shell.execute_reply":"2022-07-21T16:01:35.582953Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:35.586172Z","iopub.execute_input":"2022-07-21T16:01:35.586617Z","iopub.status.idle":"2022-07-21T16:01:35.675481Z","shell.execute_reply.started":"2022-07-21T16:01:35.586584Z","shell.execute_reply":"2022-07-21T16:01:35.673877Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# ❗ Reproducibility","metadata":{}},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:35.677690Z","iopub.execute_input":"2022-07-21T16:01:35.678234Z","iopub.status.idle":"2022-07-21T16:01:35.764803Z","shell.execute_reply.started":"2022-07-21T16:01:35.678181Z","shell.execute_reply":"2022-07-21T16:01:35.763516Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# 📈 Visualization","metadata":{}},{"cell_type":"code","source":"def visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:35.766695Z","iopub.execute_input":"2022-07-21T16:01:35.767803Z","iopub.status.idle":"2022-07-21T16:01:35.853952Z","shell.execute_reply.started":"2022-07-21T16:01:35.767765Z","shell.execute_reply":"2022-07-21T16:01:35.852961Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"gc.collect() # gc.collect() возвращает количество объектов, которые были собраны и удалены.","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:35.855470Z","iopub.execute_input":"2022-07-21T16:01:35.856050Z","iopub.status.idle":"2022-07-21T16:01:36.331470Z","shell.execute_reply.started":"2022-07-21T16:01:35.856018Z","shell.execute_reply":"2022-07-21T16:01:36.330092Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# 📦 Model\n","metadata":{}},{"cell_type":"code","source":"\nimport segmentation_models_pytorch as smp\n\n# def build_models(number):\n#     if number == 1:\n#         model_Unet = smp.Unet(\n#             encoder_name=CFG.backbone,      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation)\n#         model_Unet.to(CFG.device)\n#         CFG.models = [model_Unet]\n#     else:                  \n#         model_Unet = smp.Unet(\n#                 encoder_name=CFG.backbone,      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#                 encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#                 in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#                 classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#                 activation=CFG.activation )\n#         model_Unet.to(CFG.device)\n#         CFG.models = [model_Unet] \n\n#         model_UnetPP = smp.UnetPlusPlus(\n#             encoder_name='timm-efficientnet-b7',\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation)\n#         model_UnetPP.to(CFG.device)  \n\n#         model_inceptionresnetv2 = smp.Unet(\n#             encoder_name='timm-res2net50_26w_4s',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation )\n#         model_inceptionresnetv2.to(CFG.device)   \n\n#         model_Deep_lab = smp.UnetPlusPlus(\n#             encoder_name ='tu-gluon_xception65',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation)\n#         model_Deep_lab.to(CFG.device) \n\n\n#         model_dl2 = smp.UnetPlusPlus( \n#             encoder_name ='timm-efficientnet-b2',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation)\n#         model_dl2.to(CFG.device)   \n\n#         model_pan = smp.Unet( \n#             encoder_name ='timm-efficientnet-b2',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation)\n#         model_pan.to(CFG.device)    \n\n\n#         model_pan2 = smp.UnetPlusPlus( \n#             encoder_name ='timm-regnetx_064',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation)\n#         model_pan2.to(CFG.device) \n\n#         model_mobile = smp.Unet(\n#             encoder_name='mobilenet_v2',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#             in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#             classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n#             activation=CFG.activation)\n#         model_mobile.to(CFG.device)\n\n#         CFG.models = [model_Unet,model_UnetPP,model_inceptionresnetv2, model_Deep_lab, model_dl2, model_pan, model_pan2, model_mobile]\n#     return CFG.models\n\n    \n##################################################################################################################################################################    \n    \n##################################################################################################################################################################    \ndef build_model(indx):\n    if indx == 1: \n        # 7.7 million\n        model = smp.Unet(\n            encoder_name='efficientnet-b1',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n            encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n            in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n            classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n            activation=CFG.activation)\n        CFG.backbone = 'efficientnet-b1'\n        \n    elif indx == 2: \n        model = smp.Unet(\n            encoder_name='efficientnet-b2',\n            encoder_weights=\"imagenet\",     \n            in_channels=3,                  \n            classes=CFG.num_classes,       \n            activation=CFG.activation)\n        CFG.backbone = 'efficientnet-b2'\n        \n    elif indx == 3: \n        model = smp.Unet(\n            encoder_name='timm-mobilenetv3_small_minimal_100',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n            encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n            in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n            classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n            activation=CFG.activation)\n        CFG.backbone = 'timm-mobilenetv3_small_minimal_100'\n        \n    \n    model.to(CFG.device)\n    CFG.models = [model]\n    return  model\n\n\ndef load_models(pash):\n    for model in CFG.models:\n        model = build_model(model)\n\n\ndef load_model(path,indx):\n    model = build_model(indx)\n    model.load_state_dict(torch.load(path , map_location=torch.device('cpu')))\n    model.eval()\n    model.to(CFG.device)\n    CFG.models.append(model)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.337451Z","iopub.execute_input":"2022-07-21T16:01:36.337836Z","iopub.status.idle":"2022-07-21T16:01:36.432405Z","shell.execute_reply.started":"2022-07-21T16:01:36.337803Z","shell.execute_reply":"2022-07-21T16:01:36.431103Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading models !","metadata":{}},{"cell_type":"code","source":"preprocessing_fn =[]\n#preprocessing_fn[0] = None\n\nif CFG.JUST_PREDICT:\n    preprocessing_fn.append(None)\n    preprocessing_fn.append(smp.encoders.get_preprocessing_fn(CFG.backbone_1, CFG.weights))\n    preprocessing_fn.append(smp.encoders.get_preprocessing_fn(CFG.backbone_2, CFG.weights))\n    preprocessing_fn.append(smp.encoders.get_preprocessing_fn(CFG.backbone_3, CFG.weights))\n#################################################################################################################################\n    #preprocessing_fn[1] = smp.encoders.get_preprocessing_fn(CFG.backbone_1, CFG.weights)\n    #preprocessing_fn[2] = smp.encoders.get_preprocessing_fn(CFG.backbone_2, CFG.weights)\n    #preprocessing_fn[3] = smp.encoders.get_preprocessing_fn(CFG.backbone_3, CFG.weights)\n##########################################################################################\n\nmodel_name = [ 'Nothing','u-efficientnet-b1','u-efficientnet-b2','u-timm-mobilenetv3_small_minimal_100']\nbest_model_w = [0,CFG.best_model_w_1, CFG.best_model_w_2, CFG.best_model_w_3]\n\n#PATH = f\"best_epoch.bin\"\n#torch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.433709Z","iopub.execute_input":"2022-07-21T16:01:36.434671Z","iopub.status.idle":"2022-07-21T16:01:36.525127Z","shell.execute_reply.started":"2022-07-21T16:01:36.434620Z","shell.execute_reply":"2022-07-21T16:01:36.523861Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"\n\ndef load_img(path):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\ndef load_msk(path):\n    msk = cv2.imread(path, cv2.IMREAD_GRAYSCALE) # or msk=cv2.imread(path, 0)\n    masks = [(msk == v) for v in CFG.classes]\n    msk = np.stack(masks, axis=-1).astype('float')\n    return msk\n        \n    \ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    _transform = [\n        A.Lambda(image=preprocessing_fn),\n        A.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return A.Compose(_transform)\n\ndef get_preprocessing_test(preprocessing_fn):\n    _transform = [\n        A.Lambda(image=preprocessing_fn),\n        A.Lambda(image=to_tensor),\n    ]\n    return A.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.526712Z","iopub.execute_input":"2022-07-21T16:01:36.527740Z","iopub.status.idle":"2022-07-21T16:01:36.613949Z","shell.execute_reply.started":"2022-07-21T16:01:36.527703Z","shell.execute_reply":"2022-07-21T16:01:36.611997Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# 🍚 Dataset class\n","metadata":{}},{"cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset):\n#      \"\"\" Read images, apply augmentation and preprocessing transformations.\n#     Args:\n#         images_dir (str): path to images folder\n#         masks_dir (str): path to segmentation masks folder\n#         class_values (list): values of classes to extract from segmentation mask\n#         augmentation (albumentations.Compose): data transfromation pipeline \n#             (e.g. flip, scale, etc.)\n#         preprocessing (albumentations.Compose): data preprocessing \n#             (e.g. noralization, shape manipulation, etc.) \n\n    def __init__(self, images_paths, masks_paths = None, label=True, transforms=None,  preprocessing= None ,preprocessing_img = None ):\n        self.label      = label\n        self.img_paths  = images_paths\n        self.msk_paths  = masks_paths\n        self.transforms = transforms\n        self.preprocessing = preprocessing\n        self.preprocessing_img = preprocessing_img\n    def __len__(self):\n        return len(self.img_paths)\n    \n    \n    def __getitem__(self, index):\n        img_path  = self.img_paths[index]\n        img = load_img(img_path)\n        \n        if self.label: # WHEN WE TRAIN \n            msk_path = self.msk_paths[index]\n            msk = load_msk(msk_path)\n            \n            if self.transforms:\n                data = self.transforms(image=img, mask=msk)\n                img, msk  = data['image'], data['mask']\n            if self.preprocessing:\n                data = self.preprocessing(image=img, mask=msk)\n                img, msk  = data['image'], data['mask']\n            return img, msk\n        else: # WHEN WE PREDICT\n            if self.transforms:\n                data = self.transforms(image=img)\n                img  = data['image']\n            if self.preprocessing:\n                data =  self.preprocessing_img(image=img)\n                img = data['image']\n            return img    ","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.616212Z","iopub.execute_input":"2022-07-21T16:01:36.617320Z","iopub.status.idle":"2022-07-21T16:01:36.707180Z","shell.execute_reply.started":"2022-07-21T16:01:36.617249Z","shell.execute_reply":"2022-07-21T16:01:36.705511Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# 🍰 DataLoader","metadata":{}},{"cell_type":"code","source":"# def prepare_loaders():\n    \n#     img_names= [ os.path.join(CFG.images_path,img_name) for img_name in os.listdir(CFG.images_path)]\n#     masks_names = [ os.path.join(CFG.masks_path,mask_name) for mask_name in os.listdir(CFG.masks_path)]\n#     img_names = img_names[0:CFG.number_imgs]\n#     masks_names=masks_names[0:CFG.number_imgs]\n#     image_train, image_valid, mask_train, mask_valid = train_test_split(img_names, masks_names, test_size=0.2, random_state=CFG.seed)\n\n    \n    \n#     train_dataset = BuildDataset(image_train, mask_train, transforms=data_transforms['train'],preprocessing=get_preprocessing(preprocessing_fn))\n#     valid_dataset = BuildDataset(image_valid, mask_valid, transforms=data_transforms['valid'],preprocessing=get_preprocessing(preprocessing_fn))\n\n#     train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, \n#                               num_workers=CFG.num_workers, shuffle=True, pin_memory=True, drop_last=False)\n#     valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs, \n#                               num_workers=CFG.num_workers, shuffle=False, pin_memory=True)\n    \n#     return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.708700Z","iopub.execute_input":"2022-07-21T16:01:36.709158Z","iopub.status.idle":"2022-07-21T16:01:36.795970Z","shell.execute_reply.started":"2022-07-21T16:01:36.709086Z","shell.execute_reply":"2022-07-21T16:01:36.794350Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# train_loader, valid_loader = prepare_loaders()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.797802Z","iopub.execute_input":"2022-07-21T16:01:36.798199Z","iopub.status.idle":"2022-07-21T16:01:36.881789Z","shell.execute_reply.started":"2022-07-21T16:01:36.798166Z","shell.execute_reply":"2022-07-21T16:01:36.880560Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# imgs, msks = next(iter(train_loader))\n# imgs.size(), msks.size()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.883437Z","iopub.execute_input":"2022-07-21T16:01:36.883866Z","iopub.status.idle":"2022-07-21T16:01:36.969180Z","shell.execute_reply.started":"2022-07-21T16:01:36.883833Z","shell.execute_reply":"2022-07-21T16:01:36.967317Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:36.972447Z","iopub.execute_input":"2022-07-21T16:01:36.973936Z","iopub.status.idle":"2022-07-21T16:01:37.055678Z","shell.execute_reply.started":"2022-07-21T16:01:36.973877Z","shell.execute_reply":"2022-07-21T16:01:37.054194Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    \n    \"valid\": A.Compose([\n        A.Resize(height=CFG.start_height, width=CFG.start_width, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n\n}","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:37.057496Z","iopub.execute_input":"2022-07-21T16:01:37.058048Z","iopub.status.idle":"2022-07-21T16:01:37.141149Z","shell.execute_reply.started":"2022-07-21T16:01:37.058009Z","shell.execute_reply":"2022-07-21T16:01:37.139404Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"# Make a new directory for results !","metadata":{}},{"cell_type":"code","source":"import shutil, os\n\ntry:\n    if CFG.Kaggle:\n        os.mkdir('../working/result')\n        print('KAGGLE DIR CREATED')\n    else:\n        shutil.rmtree('./result')\n        os.mkdir('./result')\n        print('PC DIR CREATED')\nexcept Exception:\n    print(\"DIR NOT CREATED\")\n    pass\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:37.144021Z","iopub.execute_input":"2022-07-21T16:01:37.144684Z","iopub.status.idle":"2022-07-21T16:01:37.234139Z","shell.execute_reply.started":"2022-07-21T16:01:37.144646Z","shell.execute_reply":"2022-07-21T16:01:37.232825Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"# 🔭 Prediction","metadata":{}},{"cell_type":"code","source":"img_names_test= [ os.path.join(CFG.test_path,img_name) for img_name in os.listdir(CFG.test_path)]\nimg_names_test = img_names_test[0:CFG.num_test]\nprint(len(img_names_test))\n# sizes = []\n# for image_file in img_names_test:\n#     img = Image.open(image_file).convert(\"RGB\")\n#     orig_size=img.size\n#     sizes.append(orig_size)\nsizes = [(3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), \n         (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160),\n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512),\n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512),\n         (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512),\n         (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), \n         (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), \n         (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), \n         (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), \n         (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160),\n         (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), \n         (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), \n         (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), \n         (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), \n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), \n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), \n         (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), \n         (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), \n         (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), \n         (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), \n         (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512),\n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512),\n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512),\n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160),\n         (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), \n         (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), \n         (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), \n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160),\n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160),\n         (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160),\n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512),\n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160),\n         (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512),\n         (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), \n         (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), \n         (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), \n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), \n         (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512),\n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512),\n         (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512),\n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), \n         (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), \n         (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), \n         (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), \n         (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), \n         (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), \n         (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), \n         (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), \n         (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), \n         (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), \n         (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), \n         (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), \n         (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), \n         (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512),\n         (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), \n         (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), \n         (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), \n         (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), \n         (2688, 1512), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), \n         (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (2688, 1512), (3840, 2160), \n         (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512),\n         (3840, 2160), (2688, 1512), (2688, 1512), (3840, 2160), (2688, 1512), (2688, 1512), (2688, 1512), (3840, 2160), (3840, 2160), \n         (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), \n         (3840, 2160),(3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160), (3840, 2160), (3840, 2160), (3840, 2160), (2688, 1512), (3840, 2160)]\n\nsave_path = CFG.save_path\npreds = []\n\n\n            ","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:37.236018Z","iopub.execute_input":"2022-07-21T16:01:37.236544Z","iopub.status.idle":"2022-07-21T16:01:37.474781Z","shell.execute_reply.started":"2022-07-21T16:01:37.236504Z","shell.execute_reply":"2022-07-21T16:01:37.473380Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load_model(best_model_w_1,1) \n# preprocessing_fn= smp.encoders.get_preprocessing_fn('efficientnet-b1', CFG.weights)\n# test_dataset = BuildDataset(img_names_test,None, label=False, transforms=data_transforms['valid'],preprocessing=get_preprocessing(preprocessing_fn))\n# test_loader  = DataLoader(test_dataset, batch_size=1,  num_workers=CFG.num_workers, shuffle=False, pin_memory=True)\n# data_loader_iter = iter(test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:37.476346Z","iopub.execute_input":"2022-07-21T16:01:37.476744Z","iopub.status.idle":"2022-07-21T16:01:37.556533Z","shell.execute_reply.started":"2022-07-21T16:01:37.476711Z","shell.execute_reply":"2022-07-21T16:01:37.555132Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"## Color Check","metadata":{}},{"cell_type":"code","source":"###########################################################################################################################\n###########################################################################################################################\n################################################### MULTI MODEL SISTEM ####################################################\n###########################################################################################################################\n###########################################################################################################################\nif CFG.MULTIMODEL:\n    print(\"MULTI_MODEL_MODE\")\n    #for indx, model in enumerate(CFG.models, start=1):\n    for indx in range(2,3):\n        test_dataset = BuildDataset(img_names_test,None, label=False, \n                                    transforms=data_transforms['valid'],\n                                    preprocessing=get_preprocessing(preprocessing_fn[indx]), \n                                    preprocessing_img= get_preprocessing_test(preprocessing_fn[indx]))\n        test_loader  = DataLoader(test_dataset, batch_size=1,  num_workers=CFG.num_workers, shuffle=False, pin_memory=True)\n        data_loader_iter = iter(test_loader)\n\n        #imgs = imgs.to(CFG.device, dtype=torch.float)\n        #model_pred = load_model(f\"./best_epoch_of{model.name}.bin\")\n  \n        print(model_name[indx])\n        model_pred = load_model(f\"../input/russian-railways-2/best_epoch_of{model_name[indx]}_v2.bin\",indx)\n        print(f'WE USE TRAINED MODEL № {indx}: {model_pred.name}!!!')\n        model_pred = model_pred.to(CFG.device)\n        for index, imgs in enumerate(test_loader):\n            with torch.no_grad():\n                imgs = imgs.to(CFG.device, dtype=torch.float)\n                #print(imgs.shape)\n                pred = model_pred(imgs)\n                #print(pred)\n                pred = nn.Sigmoid()(pred)\n                #print(pred.shape)\n                #print(pred)\n                imgs = next(data_loader_iter)\n                #pred.to(\"cpu\",dtype=torch.float)\n                #pred = pred.cpu().detach().numpy()#(\"cpu\")\n                #pred_arg_max = np.argmax(pred, axis = 1)\n\n                #print(pred_arg_max.shape)\n                #print(pred_arg_max)\n\n                #print(np.unique(pred_arg_max))\n                # yellow - train - 10 - CLASS 2 | more light main rail - 7 - CLASS 3 | SIDE RAIL - 6 - MORE DARK CLASS 1 \\ BACKGROUND - 0 PINK CLASS 0\n                #pred_arg_max[pred_arg_max == 0 ] = 0\n                #pred_arg_max[pred_arg_max == 1 ] = 6\n                #pred_arg_max[pred_arg_max == 2 ] = 10\n                #pred_arg_max[pred_arg_max == 3 ] = 7\n                #res = np.array(pred_arg_max).astype(np.uint8)\n\n                #res = np.reshape(res, (CFG.img_size[0], CFG.img_size[1]))\n                preds.append(pred)\n                if index % CFG.print_every == 0:\n                    pred.to(\"cpu\",dtype=torch.float)\n                    pred = pred.cpu().detach().numpy()#(\"cpu\")\n                    pred_arg_max = np.argmax(pred, axis = 1)\n                    print(pred_arg_max)\n                    pred_arg_max[pred_arg_max == 0 ] = 0\n                    pred_arg_max[pred_arg_max == 1 ] = 6\n                    pred_arg_max[pred_arg_max == 2 ] = 7\n                    pred_arg_max[pred_arg_max == 3 ] = 10\n                    res = np.array(pred_arg_max).astype(np.uint8)\n                    res = np.reshape(res, (CFG.start_height,CFG.start_width))#(CFG.img_size[0], CFG.img_size[1]))\n                    img = Image.fromarray(res)\n                    img = img.resize(sizes[index],Image.NEAREST)\n                    imgplot=plt.imshow(img)\n                    plt.show()\n    results =[]\n    print(\"ANSAMBLE OF MODELS\")\n    for i in range(CFG.num_test):  \n        results =[]\n        for j in range(CFG.num_of_models):\n            results.append(preds[j*CFG.num_test+i])\n        arr = torch.stack(results, dim=0)\n        mean = torch.mean(arr,dim=0).cpu().detach().numpy()\n        pred_arg_max = np.argmax(mean, axis = 1)\n        # yellow - train - 10 - CLASS 2 | more light main rail - 7 - CLASS 3 | SIDE RAIL - 6 - MORE DARK CLASS 1 \\ BACKGROUND - 0 PINK CLASS 0\n        pred_arg_max[pred_arg_max == 0 ] = 0\n        pred_arg_max[pred_arg_max == 1 ] = 6\n        pred_arg_max[pred_arg_max == 2 ] = 7\n        pred_arg_max[pred_arg_max == 3 ] = 10\n        res = np.array(pred_arg_max).astype(np.uint8)\n        res = np.reshape(res, (CFG.start_height,CFG.start_width))#(CFG.img_size[0], CFG.img_size[1])), (CFG.img_size[0], CFG.img_size[1]))\n        img = Image.fromarray(res)\n        img = img.resize(sizes[i],Image.NEAREST)\n        if i % CFG.print_every == 0: #CFG.print_every\n            imgplot=plt.imshow(img)\n            plt.show()\n\n        img.save(save_path + img_names_test[i].split(\"/\")[-1])\ngc.collect()\nprint(\"PREDICTONS DONE !\")","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:01:37.558280Z","iopub.execute_input":"2022-07-21T16:01:37.558689Z","iopub.status.idle":"2022-07-21T16:13:54.378105Z","shell.execute_reply.started":"2022-07-21T16:01:37.558656Z","shell.execute_reply":"2022-07-21T16:13:54.376580Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# str = 'img_0.004817998680835878.png'\n# path_2 = CFG.images_path + str\n# path = CFG.masks_path + str\n# msk = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n# msk = Image.fromarray(msk)\n# imgplot=plt.imshow(msk)\n# plt.show()\n# #####################################################################\n# image = cv2.imread(path_2)\n# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# data = data_transforms['valid'](image=image)\n# image  = data['image']\n# data =  get_preprocessing_test(preprocessing_fn[3])(image=image)\n# image = data['image'] \n# image = np.expand_dims(image, axis = 0)\n# image = torch.tensor(image)\n# image = image.to(CFG.device)\n# pred = model_pred(image)\n# pred.to(\"cpu\",dtype=torch.float)\n# pred = pred.cpu().detach().numpy()#(\"cpu\")\n\n# pred_arg_max = np.argmax(pred, axis = 1)\n# # yellow - CLASS 2\n# # yellow - train - 10 - CLASS 2 | more light main rail - 7 - CLASS 3 | SIDE RAIL - 6 - MORE DARK CLASS 1 \\ BACKGROUND - 0 PINK CLASS 0\n# pred_arg_max[pred_arg_max == 0 ] = 0\n# pred_arg_max[pred_arg_max == 1 ] = 6\n# pred_arg_max[pred_arg_max == 2 ] = 7\n# pred_arg_max[pred_arg_max == 3 ] = 10\n# res = np.array(pred_arg_max).astype(np.uint8)\n# res = np.reshape(res, (CFG.start_width,CFG.start_height))#(CFG.img_size[0], CFG.img_size[1]))\n# img = Image.fromarray(res)\n# img = img.resize(sizes[index],Image.NEAREST)\n# if index % CFG.print_every == 0:\n#     imgplot=plt.imshow(img)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:13:54.382253Z","iopub.execute_input":"2022-07-21T16:13:54.383154Z","iopub.status.idle":"2022-07-21T16:13:55.451992Z","shell.execute_reply.started":"2022-07-21T16:13:54.383114Z","shell.execute_reply":"2022-07-21T16:13:55.450636Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":" print(len(preds))","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:13:55.453715Z","iopub.execute_input":"2022-07-21T16:13:55.454384Z","iopub.status.idle":"2022-07-21T16:13:55.535852Z","shell.execute_reply.started":"2022-07-21T16:13:55.454338Z","shell.execute_reply":"2022-07-21T16:13:55.534497Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# model_pred = load_model(f\"../input/rails-hackaton/best_epoch.bin\",1)\n# # try:\n# #     model_pred = load_model(f\"best_epoch.bin\",1)\n# #     print(f'WE USE TRAINED MODEL № : !!!')\n# # except Exception:\n# #     model_pred = build_model(1)\n# #     print(f'WE USE NEW MODEL 1  !!!')\n# img_1 = load_img(\"../input/russian-railways-2/test/test/img_0.007661808580294749.png\")\n# img_2 = load_img('../input/russian-railways-2/test/test/img_0.027471593748947032.png')      \n# data = data_transforms['train'](image=img_1, image2 = img_2)\n# img_1  = data['image']\n# img_2  = data['image2']\n# img_1 = np.transpose(img_1, (2, 0, 1))   \n# img_2 = np.transpose(img_2, (2, 0, 1)) \n# print(img_1)\n# print('')\n# print('')\n# print(img_2)                  \n# print(img_1.shape)\n# #imgplot=plt.imshow(img_1)\n# #plt.show()\n# #imgplot=plt.imshow(img_2)\n# #plt.show()\n# pred_1 = model_pred(img_1)\n# pred_2 = model_pred(img_2)\n# print(pred_1)\n# print(pred_2)\n# # CORREKT - MIN MAX","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:13:55.537506Z","iopub.execute_input":"2022-07-21T16:13:55.538594Z","iopub.status.idle":"2022-07-21T16:13:55.617923Z","shell.execute_reply.started":"2022-07-21T16:13:55.538552Z","shell.execute_reply":"2022-07-21T16:13:55.616527Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# path = \"../input/russian-railways-2/mask/mask/img_0.008320160156388479.png\"\n# msk = Image.open(path).convert(\"L\")\n# print(msk.size)\n# imgplot=plt.imshow(msk)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:13:55.619819Z","iopub.execute_input":"2022-07-21T16:13:55.620224Z","iopub.status.idle":"2022-07-21T16:13:55.701275Z","shell.execute_reply.started":"2022-07-21T16:13:55.620187Z","shell.execute_reply":"2022-07-21T16:13:55.700280Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"# ✂️ Remove Files","metadata":{}},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('name', 'zip', '../working/result/')\n\n#####################################\n#######         DELETE !!!  #########\n#####################################\n\n# test_path = \"../input/russian-railways-2/test/test/\"\n# img_names_test= [ os.path.join(test_path,img_name) for img_name in os.listdir(test_path)]\n# for image_file in img_names_test:\n#     try:\n#         os.remove('./result'+ img_names_test[index].split(\"/\")[-1])\n#     except Exception:\n#         pass\n# print(\"DONE!\")\n\n\n\n#for image_file in img_names_test:\n #   os.remove(\"/kaggle/working/\" + img_names_test[index].split(\"/\")[-1])","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:13:55.702659Z","iopub.execute_input":"2022-07-21T16:13:55.703373Z","iopub.status.idle":"2022-07-21T16:13:56.215819Z","shell.execute_reply.started":"2022-07-21T16:13:55.703333Z","shell.execute_reply":"2022-07-21T16:13:56.214508Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"if CFG.wandb_on:\n    !rm -r ./wandb","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:13:56.224904Z","iopub.execute_input":"2022-07-21T16:13:56.225286Z","iopub.status.idle":"2022-07-21T16:13:56.309220Z","shell.execute_reply.started":"2022-07-21T16:13:56.225255Z","shell.execute_reply":"2022-07-21T16:13:56.307877Z"},"trusted":true},"execution_count":82,"outputs":[]}]}